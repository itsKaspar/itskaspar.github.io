---
layout: post
title: "On the usage of AI-assisted targeting in the Gaza strip"
date: 2025-06-02
---

Last month I was invited to CPDP.ai (Computer Privacy, Data Protection and Artificial Intelligence) in Brussels by Privacy Salon as a participant in the CODE2025 artist program: Reclaiming Digital Agency. The days were packed with panels and everyday was about making a choice about what to see and what to miss. Among all, two panels on Palestine had specifically been targeted by the event organisers to remove the word “genocide” from their titles (or any reference to international law violation by the state of Israel)<sup><a href="">1</a></sup>. In consequence of the panelist’s refusal to do so, each of the panels were sub-texted with a disclaimer stating: “The text of this panel represents the opinions of the Panel Organiser and not those of CPDP. The case before the ICJ regarding the categorization of Israel’s activities in Gaza has yet to be decided.” Needless to say, the panels were packed and were by far the most interesting of the three-day event. Following is a compilation of news sources and personal reflections, inspired by these panels that I wanted to put in writing in order to better grasp the current state of AI in warfare.

In traditional military terms, the act of bombing refers to dropping unguided bombs from an aircraft. However modern usage has expanded, operations described as “bombing campaigns” more often include precisely guided cruise missiles and drone-launched missiles. The expansion of the term reflects on how the tactical means are often disconnected from the actual effects. To be legal in regards of international law, these operations must comply with the principles of humanitarian law: 1- military necessity (only force required to achieve legitimate military objectives), 2- distinction (must differentiate between combatants and civilians), and 3- proportionality (anticipated military advantage must outweigh expected civilian harm). Humanitarian law is a way to ensure that military operations do not engage in acts of terror bombing; bombing of civilian targets without military value in the hope of damaging an enemy’s morale<sup>2</sup>. These legal obligations create a practical pressure which influences the structure of decision making in military organizations. For example, the requirement for “distinction” means that intelligence must be gathered before identifying targets. Whereas “proportionality” demands some form of collateral damage assessment, and so on.

Traditionally, identifying a target can take weeks or months as it involves a multi-layered intelligence process. Analysts will collect intelligence from various sources such as satellite data or intercepted communication which is then cross-referenced and verified in order to build a comprehensive target profile. Working jointly, a group of analysts might suggest 50 targets a year<sup>3</sup>. While I am unfamiliar with the specifics of these layers as they vary according to each military institution, in an ideal world, this information is then passed on through legal officers whose role is to verify the targets compliance with humanitarian law. If approved the target is then passed on to operators on the field who integrate the target within their tactical agenda. Until finally, the bomb or missile is launched. This chain of command embodies the dual nature of military bureaucracy, in which strikes, are both enabled and limited by the same hierarchy. But the lack of targets isn’t always an injunction to stop bombing. A report by investigative journalist Yuval Abraham from +972 Magazine, states that during the 2014 Gaza war, the Israel Defence Forces (IDF) ran out of targets to bomb, but somehow kept bombing. Under political pressure to continue the war, they decided to maintain pressure by bombing the same places multiple times<sup>4</sup>. 

Since, the use of AI tools has rapidly become an integrant part of the chain of command. Among them, two computer programs named ”The Gospel” and “Lavender” are described as decision-support tools and have been facilitating military analyst’s jobs in finding valid targets. According to Aviv Kovachi, the head of the IDF until 2023, the first time the Gospel was activated it generated 100 targets a day. As of April 2025, about 100 000 tons of bombs has been fired into the Gaza strip. To put this number into perspective, in 1945 the atomic bomb dropped on Hiroshima had a yield of 15 000 tons. For Gaza, that is almost 7 times more for an area 2,5x times smaller<sup>5</sup>. These numbers are far higher than previous conflicts within the same region, and can be explained both by the higher number of generated targets and by the reduction of friction within the process of target verification. In 2024, an operative working as an analyst and whose job is to verify AI targets, said in an interview to the Guardian “I would invest 20 seconds for each target at this stage, and do dozens of them every day. I had zero added-value as a human, apart from being a stamp of approval. It saved a lot of time” <sup>6</sup>.

The absence of thorough human implication poses a significant issue here, as the nature of AI systems is to provide content based on statistical and probability inference, rather than reasoning and factual evidence <sup>7</sup>. Lavender for instance, is an AI system that generates human targets. It assigns a score to each Palestinian, a rating between 1 and 100, ranking how apparently likely they can be affiliated with Hamas operatives<sup>8</sup>. In confidence-based classification systems, the reduction of complex human identities to a single probability score, obscures the underlying reality. Just like an image classification model might assign probabilities over categories (80% cat, 10% dog, 5%plant,..), the end decision of the classifier is set by a threshold, which collapses any profile above or under it, between “militant” and “not militant”, a binary classification. While classifiers confidence seems reasonable for high scoring values, the lower you set the system’s threshold, the higher is the risk of classifying subjects with low confidence scores. Unfortunately military executives have been tampering with these systems, asking to lower the thresholds, inherently lowering the score which classifies people as Hamas operative. Meaning that a whole new range of civilians which have looser connections with militants could themselves be considered as militants (friends, family, neighbours,..). In an investigation on Lavender by +972, a senior officer shared: “In a day without targets whose feature rating was sufficient to authorize a strike, we attacked at a lower threshold. We were constantly being pressured”<sup>9</sup>. 

Many of the concerns I raise can be summed up in this question: Are AI systems used in warfare replacing the role of human analysts, or are they extending the tools of war, by creating new means of waging war? Journalist Yuval Abraham has compared the integration of AI systems in warfare to a “mass assassination factory”, which is on par with the idea these AI systems function not only as accelerators of war, but also as target generators, tunable on demand, that justify a never-ending stream of new targets, diluting any form of human accountability through their impenetrable complexity. But although it feels like tomorrow's wars will be fully automated, it is important to remind ourselves that there are humans on both sides of these machines, from the killing to the killed. For instance, even when executing civilians, the operators and the AI targeting system are working exactly as intended. For each target, operatives get an indicator which resembles a traffic light system (green, orange, red) to grasp the risk for civilians lives of each strike. According to testimonies from intelligence officers, commanders rely on even more specific predetermined and fixed collateral damage degree for each strike, with specific numbers: "15–20 civilians casualties allowed per 'low-ranking militant' and over 100 civilians for a more senior commander"<sup>10</sup>. As predicted errors and casualties have become statistical, they are now seen merely as operating costs of war. As another source said in an interview for +972 Magazine: “Nothing happens by accident”<sup>11</sup>, the casualties are always known in advance of each strike. Except maybe for the overall 10% error rate of Lavender, a number measured and considered acceptable by the IDF itself: once out of ten, the wrong target is hit, taking along with it, the lives of civilians who were around  at the time of the strike<sup>9</sup>.

Though the subject of this article was originally to address only AI-assisted targeting tools, I must also outline what other AI tools are used to fully grasp the extent of war automation and friction reduction in military decision loops. The discussed loop here is the OODA loop, which stands for Observe, Orient, Decide and Act. It is a classic decision-making model widely used in occidental military operations<sup>12</sup>. The efficiency and implications of any military operation is understood through the analysis of the elements in one’s loop and the friction between these. At the first two steps “Observe” and “Orient”, the two main actors are the AI models described earlier: “The Gospel” to determine building targets, and “Lavender” to determine human targets. At the second step “Orient”, we can also find another AI tool named (with misplaced irony) “Where’s Daddy”, whose role is to notify when a surveilled individual enters a designated area. For example, it can be used to alert when someone marked by Lavender is at home, sleeping in their bedroom, making them an easier target<sup>13</sup>. At the third step of the loop “Decide”, is “Fire Factory” a management AI that uses the data from previous steps, to calculate logistics such as the needed munition load, and assign the target to a close-by aircraft or drone, along with a tactical schedule. It does all this within a matter of minutes<sup>14</sup>. Finally on the fourth and last step of the loop “Act”, it is not an AI system, but rather another symptomatic product of digitization, a phone application called “Pillars of Fire”, upon which on-ground commanders receive their missions via military issued smartphones.

The proposed analysis of the AI systems used within the IDF’s OODA loop might be incomplete as it relies on a few inside sources and investigations rather than a full disclosure. In further news articles we may learn about the existence and usage of other tools which will likely extend the current implications. Although we lack this information today, a clear outline has been drawn about the IDF’s systemic intentions and how they have decided to wage war. By compressing the OODA loop to its maximum, the IDF has drastically reduced the time it takes from intelligence gathering to bombing to mere minutes, with as little as 20 seconds of human oversight in some cases. By prioritising operational speed at the expense of civilian lives, they have infringed humanitarian law, eroded not only their enemy’s humanity but their own. Whilst the automated killing has been characterized as a “mass assassination factory”, this metaphor fails to capture how the deliberate manipulation of targeting thresholds to generate even more targets represents systemised institutional corruption. The convergence of AI probabilistic analysis along with human manipulation of these same systems has united all the technical conditions for genocide.

- [1] Access Now, CPDP must resist pressure https://www.accessnow.org/press-release/cpdp-must-resist-pressure-muzzle-discussions-palestine/
- [2] https://en.wikipedia.org/wiki/Strategic_bombing
- [3] https://www.ynetnews.com/magazine/article/ry0uzlhu3
- [4] Yuval Abraham, https://www.france24.com/en/tv-shows/perspective/20231212-understanding-how-israel-uses-gospel-ai-system-in-gaza-bombings
- [5] https://republic.us/local-news/lt-colonel-william-astore-a-grim-reminder-about-gaza-100-kilotons-is-roughly-seven-hiroshimas/20517
- [6] https://www.theguardian.com/world/2024/apr/03/israel-gaza-ai-database-hamas-airstrikes
- [7] https://www.npr.org/2023/12/14/1218643254/israel-is-using-an-ai-system-to-find-targets-in-gaza-experts-say-its-just-the-st
- [8] https://tempestmag.org/2024/07/the-lavender-program/
- [9] https://www.972mag.com/lavender-ai-israeli-army-gaza/
- [10] https://www.rusi.org/explore-our-research/publications/commentary/israel-defense-forces-use-ai-gaza-case-misplaced-purpose
- [11] https://www.972mag.com/mass-assassination-factory-israel-calculated-bombing-gaza/
- [12] https://en.wikipedia.org/wiki/OODA_loop
- [13] https://www.lemonde.fr/en/international/article/2024/04/05/israeli-army-uses-ai-to-identify-tens-of-thousands-of-targets-in-gaza_6667454_4.html
- [14] https://www.bloomberg.com/news/articles/2023-07-16/israel-using-ai-systems-to-plan-deadly-military-operations